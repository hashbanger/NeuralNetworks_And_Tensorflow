{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_in_Tensorflow_CIFAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IdbW9CNQFpOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Convolutional Neural Network to Classify the CIFAR dataset"
      ]
    },
    {
      "metadata": {
        "id": "nBF1rc4VFpOM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Prashant Brahmbhatt](www.github.com/hashbanger)"
      ]
    },
    {
      "metadata": {
        "id": "MKZlpfZqFpOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:  \n",
        "- [Deep-Diver Github](https://github.com/deep-diver/)  \n",
        "- [CIFAR-10 official Documentation](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "metadata": {
        "id": "YqfAbheMFpOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___"
      ]
    },
    {
      "metadata": {
        "id": "I6wmA428FpOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffc3ff5c-864e-4b57-b2be-ed6bee591a27"
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm \n",
        "import tarfile\n",
        "\n",
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
        "\n",
        "class DownloadProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "\"\"\" \n",
        "    check if the data (zip) file is already downloaded\n",
        "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
        "\"\"\"\n",
        "if not isfile('cifar-10-python.tar.gz'):\n",
        "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
        "        urlretrieve(\n",
        "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
        "            'cifar-10-python.tar.gz',\n",
        "            pbar.hook)\n",
        "\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CIFAR-10 Dataset: 171MB [01:32, 1.84MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XLcz4Q02FpO6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset consists of 10 object classes dogs, cats, aeroplanes, automobile, deer, horse, ship, truck, frog, bird.  \n",
        "We will require the labels to be one-hot encoded and the images require to be normalized."
      ]
    },
    {
      "metadata": {
        "id": "9eOokxC9FpPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ius4PXWcFpPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data files.\n",
        "The data archive would be extracted in 6 data files named as \n",
        "    - data_batch_i (1,2,...5)\n",
        "    - test_batch"
      ]
    },
    {
      "metadata": {
        "id": "w1i_P70hFpPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Knowing the data"
      ]
    },
    {
      "metadata": {
        "id": "iDfLSktlFpPk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset contains coloured images of dimensions 32 x 32.  \n",
        "As the documentation states that the data files contains numpy arrays of (10000 * 3072) in row major order  \n",
        "32 height x 32 width x 3 channels RGB = 3072  \n",
        "There are 10000 images per data_batch = 50000 train images  \n",
        "10000 test images.  \n",
        "The first 1024 channels are for red channel ,next for green channel and then blue.\n",
        "\n",
        "The provided dimensions are not suitable to be fetched into the tensorflow so we will need to shape the data as the **tf.nn.conv2d()** requires which is either  \n",
        "\n",
        "**[batch , height , width , num_of_channels]**  or an alternative form could be  \n",
        "\n",
        "**[batch , num_of_channels , height , width]**"
      ]
    },
    {
      "metadata": {
        "id": "EkYq3LlCFpPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Labels\n",
        "\n",
        "The original order of the data labels are:  \n",
        "\n",
        "**0 airplane \t\t\t\t\t\t\t\t\t\t\n",
        "1 automobile \t\t\t\t\t\t\t\t\t\t\n",
        "2 bird \t\t\t\t\t\t\t\t\t\t\n",
        "3 cat \t\t\t\t\t\t\t\t\t\t\n",
        "4 deer \t\t\t\t\t\t\t\t\t\t\n",
        "5 dog \t\t\t\t\t\t\t\t\t\t\n",
        "6 frog \t\t\t\t\t\t\t\t\t\t\n",
        "7 horse \t\t\t\t\t\t\t\t\t\t\n",
        "8 ship \t\t\t\t\t\t\t\t\t\t\n",
        "9 truck**"
      ]
    },
    {
      "metadata": {
        "id": "6DFgZskRFpPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_label_names():\n",
        "    return['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92KBatbWFpP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reshaping the data"
      ]
    },
    {
      "metadata": {
        "id": "pJoRaLhTFpQN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will firstly use the **numpy.reshape()** to shape our images and using the **np.traspose()** function.  \n",
        "The reshaping will be done in two steps:  \n",
        "- First we will convert the row vector 3072  into 3 x 1024, as per the channel.  \n",
        "- Secondly, we will convert it to 3 x 32 x 32 by dividing the tensor resulting from the first reshape by 32 where 32 is the width of the image because of the row major order."
      ]
    },
    {
      "metadata": {
        "id": "5SOczlf1FpQQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The reshape function will be called with parameters, (10000, 3, 32, 32).  \n",
        "\n",
        "Now, the data is represented as **(num_channel , width , height)** form. But tensorflow and matplotlib expect a different shape. They expect **(width , height , num_channel)** instead. We need to swap the order using the transpose.  \n",
        "\n",
        "The transpose function can take a list of axes, and each value specifies where it wants to move around. For example, calling transpose with argument **(1, 2, 0)** in an numpy array of **(num_channel, width, height)** will return a new numpy array of **(width, height, num_channel)**."
      ]
    },
    {
      "metadata": {
        "id": "zfQ1PMf9FpQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
        "        # note the encoding type is 'latin1'\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "        \n",
        "    features = batch['data'].reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        " #Here we can use len(batch['data'] in place of -1 but -1 automatically considers the size of the batch\n",
        "    \n",
        "    labels = batch['labels']\n",
        "        \n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UqCbcgpRFpQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Exploring"
      ]
    },
    {
      "metadata": {
        "id": "k3QjwBYRFpQp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below code cells are used to know about the data and its stats."
      ]
    },
    {
      "metadata": {
        "id": "ATR9BLLvFpQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_stats(cifar_dataset_folder_path, batch_id, sample_id):\n",
        "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
        "    \n",
        "    if not 0 <= sample_id < len(features):\n",
        "        print(\"{} samples in batch {} .{} is out of range\".format(len(features), batch_id, sample_id))\n",
        "        return None\n",
        "    \n",
        "    print(\"\\nBatch {} Stats\\n\".format(batch_id))\n",
        "    print(\"Number of samples: {}\\n\".format(len(features)))\n",
        "    label_names = load_label_names()\n",
        "    label_counts =  dict(zip(*np.unique(labels, return_counts = True))) \n",
        "    #return_count will return number of the counts and a second tuple will be returned with positional counts\n",
        "    #the starred expression here will automatically map the two returned tuples as arguments of zip\n",
        "    #for starred expression docs refer (https://docs.python.org/dev/reference/expressions.html#calls)\n",
        "    \n",
        "    for key, value in label_counts.items(): #.items() returns a tuple of the key value pairs\n",
        "        print('Counts of {} {} are {}'.format(key, label_names[key], value))\n",
        "        \n",
        "    sample_image = features[sample_id]\n",
        "    sample_label = labels[sample_id]\n",
        "    \n",
        "    print('Example of image {}'.format(sample_id))\n",
        "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
        "    print('Image - Shape:{}'.format(sample_image.shape))\n",
        "    print('Label - Label _id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
        "    \n",
        "    plt.imshow(sample_image)    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BncN8JgMFpRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "ad3c6472-db41-4586-d0dd-20dfbc20008e"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "batch_id = 2\n",
        "sample_id = 6000\n",
        "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2 Stats\n",
            "\n",
            "Number of samples: 10000\n",
            "\n",
            "Counts of 0 airplane are 984\n",
            "Counts of 1 automobile are 1007\n",
            "Counts of 2 bird are 1010\n",
            "Counts of 3 cat are 995\n",
            "Counts of 4 deer are 1010\n",
            "Counts of 5 dog are 988\n",
            "Counts of 6 frog are 1008\n",
            "Counts of 7 horse are 1026\n",
            "Counts of 8 ship are 987\n",
            "Counts of 9 truck are 985\n",
            "Example of image 6000\n",
            "Image - Min Value: 5 Max Value: 182\n",
            "Image - Shape:(32, 32, 3)\n",
            "Label - Label _id: 7 Name: horse\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAHxCAYAAAB5x1VAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe0797d3668>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 249,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YCC7Jc6dFpRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing The Inputs"
      ]
    },
    {
      "metadata": {
        "id": "NCrdP3gZFpRn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalization"
      ]
    },
    {
      "metadata": {
        "id": "INNsyEqrFpRr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to make all the input values for al the images between 0 and 1.  \n",
        "We use the Min-Max Scaling.  \n",
        "\n",
        "#### Min Max Normalization\n",
        "Minmax normalization is a normalization strategy which linearly transforms x to y   \n",
        "## $$y = (x-min)/(max-min)$$  \n",
        "Where min and max are the minimum and maximum values in X, where X is the set of observed values of x.  \n",
        "It can be easily seen that when x=min, then y=0, and\n",
        "When $x=max$, then $y=1$.\n",
        "This means, the minimum value in X is mapped to 0 and the maximum value in X is mapped to 1. So, the entire range of values of X from min to max are mapped to the range 0 to 1."
      ]
    },
    {
      "metadata": {
        "id": "IFuB65TubyVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Why normalization should be performed is somewhat related to activation function.  \n",
        "Sigmoid activation function takes an input value and outputs a new value ranging from 0 to 1. When the input value is somewhat large, the output value easily reaches the max value 1. Similarily, when the input value is somewhat small, the output value easily reaches the max value 0.\n",
        "ReLU activation function takes an input value and outputs a new value ranging from 0 to infinity. When the input value is somewhat large, the output value increases linearly. However, when the input value is somewhat small, the output value easily reaches the max value 0.  \n",
        "\n",
        "The images are in an input range from 0 to 255 so no normalization is likely to result in 1 most of the times which would not be appropriate. So we define a normalize function."
      ]
    },
    {
      "metadata": {
        "id": "3bmX18zbb4iw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    \"input the numpy array to be normalized\"\n",
        "    min_ = np.min(x) #the conventional min() would not work here\n",
        "    max_ = np.max(x)\n",
        "    y = (x - min_)/(max_ - min_ )\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYdESURBb9zf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see if our function works correct"
      ]
    },
    {
      "metadata": {
        "id": "xDro41ibbzHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1a04c906-6c06-4cc8-a42b-27bcbe7dbcf4"
      },
      "cell_type": "code",
      "source": [
        "normalize( np.array([[[3,3,3],[1,2,3],[4,5,6]]]) )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.4, 0.4, 0.4],\n",
              "        [0. , 0.2, 0.4],\n",
              "        [0.6, 0.8, 1. ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "lXb6vgDvcC48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like the function is working well with our sample 3D array."
      ]
    },
    {
      "metadata": {
        "id": "drttTdxYb8js",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One hot encoding"
      ]
    },
    {
      "metadata": {
        "id": "qgGs6cAIcL8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since for each image the model would require to match its output probabilities for each of the 10 classes with the ground truth. But our label is only a single value so we have to modify it in a 1D vector of size 10.\n",
        "\n",
        "Example:** 3 ---one-hot---->  [0,0,0,1,0,0,0,0,0,0] **"
      ]
    },
    {
      "metadata": {
        "id": "bqPodqH3cJym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(x):\n",
        "    \"Takes numpy array and One Hot Encodes\"\n",
        "    encoded = np.zeros((len(x), 10))\n",
        "    \n",
        "    for idx, val in enumerate(x):\n",
        "        encoded[idx][val] = 1\n",
        "        \n",
        "    return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5-cwWaqcQAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we use all the above defined functions and preprocess all the input data and save it as a file.  \n",
        "We will also use 90% inputs from each batch during the training ad the rest of the 10% will be used as the validation set."
      ]
    },
    {
      "metadata": {
        "id": "ZNLyhPjKcNfE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Not working\n",
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "    \n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "    \n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        \n",
        "        #we use the 10% of the input of each batch to be reserved for validation\n",
        "        #So we preprocess all the training data and normalize the validation all stacked together\n",
        "        index_of_validation = int(len(features) * 0.1)\n",
        "        \n",
        "        #Preprocess 90%\n",
        "        # noramlize\n",
        "        # one hot encode\n",
        "        # save it as new file\n",
        "        # save file per batch\n",
        "        _preprocess_and_save(normalize, one_hot_encode, features[:-index_of_validation], \n",
        "                           labels[:-index_of_validation], 'preprocess_batch_'+ str(batch_i)+ '.p')\n",
        "        \n",
        "        valid_features.extend(features[-index_of_validation:])\n",
        "        valid_labels.extend(features[-index_of_validation:])\n",
        "        \n",
        "    #preprocess all the stacked validation set\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(valid_features), np.array(valid_labels),\n",
        "                         'preprocess_validation.p')\n",
        "\n",
        "   \n",
        "    #load the test set\n",
        "    with open(cifar10_dataset_folder_path+'/test_batch', mode = 'rb') as file:\n",
        "        batch = pickle.load(file,  encoding= 'latin1')\n",
        "        \n",
        "    #reshaping the test set\n",
        "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "    \n",
        "    #preprocess and save all the test set\n",
        "    _preprocess_and_save(normalize, one_hot_encode, np.array(test_features), np.array(test_labels)\n",
        "                        ,'preprocess_training.p')\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdylgsHutiJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        \n",
        "        #we use the 10% of the input of each batch to be reserved for validation\n",
        "        #So we preprocess all the training data and normalize the validation all stacked together\n",
        "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
        "        index_of_validation = int(len(features) * 0.1)\n",
        "\n",
        "        #Preprocess 90%\n",
        "        # noramlize\n",
        "        # one hot encode\n",
        "        # save it as new file\n",
        "        # save file per batch\n",
        "        _preprocess_and_save(normalize, one_hot_encode,\n",
        "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
        "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
        "\n",
        "        \n",
        "        valid_features.extend(features[-index_of_validation:])\n",
        "        valid_labels.extend(labels[-index_of_validation:])\n",
        "\n",
        "    #preprocess all the stacked validation set\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(valid_features), np.array(valid_labels),\n",
        "                         'preprocess_validation.p')\n",
        "\n",
        "    # load the test dataset\n",
        "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # reshaping the test set\n",
        "    test_features = batch['data'].reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "\n",
        "    #preprocess and save all the test set\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(test_features), np.array(test_labels),\n",
        "                         'preprocess_training.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OAFek3bNcUuK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvSRTPkv_I4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___________________"
      ]
    },
    {
      "metadata": {
        "id": "tUd59rowwk9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode = 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8krCgjgS_Nbt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the Network"
      ]
    },
    {
      "metadata": {
        "id": "FnpHc61L_LlQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Removing the previous weights, biases, inputs from memory\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#Input\n",
        "x = tf.placeholder(tf.float32, shape = (None, 32, 32, 3), name = 'input_x')\n",
        "y = tf.placeholder(tf.float32, shape = (None, 10), name = 'output_y')\n",
        "keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFTpJEbr_S9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Network Layout  "
      ]
    },
    {
      "metadata": {
        "id": "i-0fKWIn_We9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The entire model consists of 14 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
        "\n",
        "- Convolution Layer of dimension 3 x 3 x 64  (where 64 being the number of filters)  \n",
        "ReLU activation function  \n",
        "Max Pooling by 2    \n",
        "Batch Normalization   \n",
        "\n",
        "    \n",
        "- Convolution Layer of dimension 3 x 3 x 128    \n",
        "ReLU activation function    \n",
        "Max Pooling by 2    \n",
        "Batch Normalization\n",
        "    \n",
        "    \n",
        "- Convolution Layer of dimension 3 x 3 x 256  \n",
        "ReLU activation function  \n",
        "Max Pooling by 2    \n",
        "Batch Normalization\n",
        "\n",
        "\n",
        "- Convolution Layer of dimension 3 x 3 x 512  \n",
        "ReLU activation function  \n",
        "Max Pooling by 2    \n",
        "Batch Normalization\n",
        "\n",
        "\n",
        "- Flattening output of the last convolutional layer.\n",
        "\n",
        "\n",
        "- Full connection of 128 units  \n",
        "Dropout  \n",
        "Batch Normalization  \n",
        "\n",
        "\n",
        "- Full connection of 256 units  \n",
        "Dropout  \n",
        "Batch Normalization  \n",
        "\n",
        "\n",
        "- Full connection of 512 units  \n",
        "Dropout  \n",
        "Batch Normalization  \n",
        "\n",
        "\n",
        "- Full connection of 1024 units  \n",
        "Dropout  \n",
        "Batch Normalization  \n",
        "\n",
        "\n",
        "- Full connection of 10 units (number of image classes)\n"
      ]
    },
    {
      "metadata": {
        "id": "KIyoynBu_cQd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References to [batch normalization](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)"
      ]
    },
    {
      "metadata": {
        "id": "uw8upu1m_QYC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_net(x, keep_prob):\n",
        "    \n",
        "    conv1_filter = tf.Variable(tf.truncated_normal(shape = [3, 3, 3, 64], mean= 0, stddev= 0.08))\n",
        "    conv1 = tf.nn.conv2d(x, conv1_filter, [1, 1, 1, 1], padding= 'SAME')\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    conv1_pool = tf.nn.max_pool(conv1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding= 'SAME')\n",
        "#    conv1_bn = tf.nn.batch_normalization(conv1_pool)\n",
        "    \n",
        "    conv2_filter = tf.Variable(tf.truncated_normal(shape = [3, 3, 64, 128], mean= 0, stddev= 0.08))\n",
        "    conv2 = tf.nn.conv2d(conv1_pool, conv2_filter, [1, 1, 1, 1], padding= 'SAME')\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    conv2_pool = tf.nn.max_pool(conv2, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding= 'SAME')\n",
        "#    conv2_bn = tf.nn.batch_normalization(conv2_pool)\n",
        "    \n",
        "    conv3_filter = tf.Variable(tf.truncated_normal(shape = [5, 5, 128, 256], mean= 0, stddev= 0.08))\n",
        "    conv3 = tf.nn.conv2d(conv2_pool, conv3_filter, [1, 1, 1, 1], padding= 'SAME')\n",
        "    conv3 = tf.nn.relu(conv3)\n",
        "    conv3_pool = tf.nn.max_pool(conv3, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding= 'SAME')\n",
        "#    conv3_bn = tf.nn.batch_normalization(conv3_pool)\n",
        "    \n",
        "    conv4_filter = tf.Variable(tf.truncated_normal(shape = [5, 5, 256, 512], mean= 0, stddev= 0.08))\n",
        "    conv4 = tf.nn.conv2d(conv3_pool, conv4_filter, [1, 1, 1, 1], padding= 'SAME')\n",
        "    conv4 = tf.nn.relu(conv4)\n",
        "    conv4_pool = tf.nn.max_pool(conv4, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding= 'SAME')\n",
        "#    conv4_bn = tf.nn.batch_normalization(conv4_pool)  \n",
        "    \n",
        "    flat = tf.contrib.layers.flatten(conv4_pool)\n",
        "    \n",
        "    full1 = tf.contrib.layers.fully_connected(inputs = flat, num_outputs = 128, activation_fn = tf.nn.relu)\n",
        "    full1 = tf.nn.dropout(full1, keep_prob)\n",
        "#    full1 = tf.layers.batch_normalization(full1)\n",
        "    \n",
        "    full2 = tf.contrib.layers.fully_connected(inputs = full1, num_outputs = 256, activation_fn = tf.nn.relu)\n",
        "    full2 = tf.nn.dropout(full2, keep_prob)\n",
        "#    full2 = tf.layers.batch_normalization(full2)\n",
        "    \n",
        "    full3 = tf.contrib.layers.fully_connected(inputs = full2, num_outputs = 512, activation_fn = tf.nn.relu)\n",
        "    full3 = tf.nn.dropout(full3, keep_prob)\n",
        "#    full3 = tf.layers.batch_normalization(full3)\n",
        "    \n",
        "    full4 = tf.contrib.layers.fully_connected(inputs = full3, num_outputs = 1024, activation_fn = tf.nn.relu)\n",
        "    full4 = tf.nn.dropout(full4, keep_prob)\n",
        "#    full4 = tf.layers.batch_normalization(full4)\n",
        "    \n",
        "    out = tf.contrib.layers.fully_connected(inputs = full4, num_outputs = 10, activation_fn = None)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGi-P1Pk_f4d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "ewJDw_vz_d_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "keep_probability = 0.7\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DGE-10QQ_k5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cost Optimization and Loss Functions"
      ]
    },
    {
      "metadata": {
        "id": "HWtPh_r7_h3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = conv_net(x, keep_prob)\n",
        "model = tf.identity(logits, name = 'logits')\n",
        "\n",
        "#Loss function and Optimization\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits= logits, labels = y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(cost)\n",
        "\n",
        "#Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__jogxihFEzh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Network"
      ]
    },
    {
      "metadata": {
        "id": "4jP0TNXnFHcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single Optimization"
      ]
    },
    {
      "metadata": {
        "id": "rhGl6BM2_ohd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
        "    session.run(optimizer, feed_dict = {x: feature_batch, y: label_batch, keep_prob: keep_probability})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EwZwlzDFNs4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show Stats"
      ]
    },
    {
      "metadata": {
        "id": "FKaZTVqeFLLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
        "    loss = sess.run(cost, feed_dict = {x: feature_batch, y:label_batch, keep_prob: 1})\n",
        "    \n",
        "    valid_acc = sess.run(accuracy, feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1})\n",
        "    \n",
        "    print('Loss: {:10.4f} Validation Accuracy:{:.6f}'.format(loss, valid_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2JRniJWFTPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fully Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "jj8e7LKTFPcI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_feature_labels(features, labels, batch_size):\n",
        "    \"\"\"Split features and labels into batches\"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start +batch_size , len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "    \n",
        "def load_preprocess_training_batch(batch_id, batch_size):\n",
        "    \"\"\"Load the Preprocessed Training data and return them in batches of <batch_size> or less\"\"\"\n",
        "    filename = 'preprocess_batch_'+ str(batch_id) + '.p'\n",
        "    features, labels = pickle.load(open(filename, mode = 'rb'))\n",
        "    \n",
        "    # Return the training data in batches of size <batch_size> or less\n",
        "    return batch_feature_labels(features, labels, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiH_RCYDFYww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "891da887-2be5-4072-9776-7ede52a9ed70"
      },
      "cell_type": "code",
      "source": [
        "save_model_path = './image_classification'\n",
        "\n",
        "print('Training...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        # Loop over all batches\n",
        "        n_batches = 5\n",
        "        for batch_i in range(1, n_batches + 1):\n",
        "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "                \n",
        "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "            \n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    save_path = saver.save(sess, save_model_path)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2636 Validation Accuracy:0.152000\n",
            "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.9547 Validation Accuracy:0.183600\n",
            "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.7651 Validation Accuracy:0.199800\n",
            "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.7986 Validation Accuracy:0.274600\n",
            "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.6305 Validation Accuracy:0.262000\n",
            "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8135 Validation Accuracy:0.253200\n",
            "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.7203 Validation Accuracy:0.272800\n",
            "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.5422 Validation Accuracy:0.336200\n",
            "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.5171 Validation Accuracy:0.386400\n",
            "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.4735 Validation Accuracy:0.340000\n",
            "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.3993 Validation Accuracy:0.440000\n",
            "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.2263 Validation Accuracy:0.481400\n",
            "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.0714 Validation Accuracy:0.472600\n",
            "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.1444 Validation Accuracy:0.541000\n",
            "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.9910 Validation Accuracy:0.554400\n",
            "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.8031 Validation Accuracy:0.604400\n",
            "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.7217 Validation Accuracy:0.605800\n",
            "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.7157 Validation Accuracy:0.589200\n",
            "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.8046 Validation Accuracy:0.635800\n",
            "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.6310 Validation Accuracy:0.647800\n",
            "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.5486 Validation Accuracy:0.651000\n",
            "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.4939 Validation Accuracy:0.670200\n",
            "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.3759 Validation Accuracy:0.676400\n",
            "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.4535 Validation Accuracy:0.681600\n",
            "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.4886 Validation Accuracy:0.688800\n",
            "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.3326 Validation Accuracy:0.684600\n",
            "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.3940 Validation Accuracy:0.679600\n",
            "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.1466 Validation Accuracy:0.691600\n",
            "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.3211 Validation Accuracy:0.698600\n",
            "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.2275 Validation Accuracy:0.688800\n",
            "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.2066 Validation Accuracy:0.711000\n",
            "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.2582 Validation Accuracy:0.682000\n",
            "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.1287 Validation Accuracy:0.700800\n",
            "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.2205 Validation Accuracy:0.698200\n",
            "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.1706 Validation Accuracy:0.699400\n",
            "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.0808 Validation Accuracy:0.720800\n",
            "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.0949 Validation Accuracy:0.696600\n",
            "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.0862 Validation Accuracy:0.694200\n",
            "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.1476 Validation Accuracy:0.683200\n",
            "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.1250 Validation Accuracy:0.701200\n",
            "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.1854 Validation Accuracy:0.694600\n",
            "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.0793 Validation Accuracy:0.676400\n",
            "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0458 Validation Accuracy:0.710600\n",
            "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.1390 Validation Accuracy:0.708000\n",
            "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.0581 Validation Accuracy:0.711000\n",
            "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0319 Validation Accuracy:0.714400\n",
            "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0403 Validation Accuracy:0.685800\n",
            "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0264 Validation Accuracy:0.697000\n",
            "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0427 Validation Accuracy:0.706600\n",
            "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0337 Validation Accuracy:0.698600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrYcQQMgFcty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "aa4d5984-e415-4907-e240-6d61fca5f220"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3YHsfrj1Huxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSpwkPDsJGC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## de nada!"
      ]
    }
  ]
}
